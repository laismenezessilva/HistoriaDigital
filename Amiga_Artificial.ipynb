{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdEudV97-Lwe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj2no-reELmT"
      },
      "source": [
        "Desenvolva um produto prático utilizando uma das ferramentas ou métodos apresentados na\n",
        " disciplina (ex: Zotero, Omeka, Tropy, StoryMap, OCR, NLP, GitHub, IA generativa, etc).\n",
        " O trabalho deve incluir:\n",
        " �. Produto final – site, visualização, protótipo, script, exposição virtual, base de dados, relatório\n",
        " ou outro formato digital.\n",
        " �. Relatório metodológico (2–3 páginas) explicando:\n",
        " ◦ Objetivo e contexto do trabalho;\n",
        " ◦ Ferramentas e métodos utilizados;\n",
        " ◦ Fontes e dados empregados;\n",
        " ◦ Dificuldades e soluções encontradas;\n",
        " ◦ Reflexão crítica sobre o processo.\n",
        "Critérios de Avaliação\n",
        "\n",
        "\n",
        "Trabalho apresentado pela aluna Laís Menezes de Souza Silva para obtenção parcial de nota na disciplina Introdução à História Digital, do Programa de Pós-Graduação Interdisciplinar em Humanidades Digitais do Instituto Multidisciplinar da Universidade Federal Rural do Rio de Janeiro, ministrada pelo Prof. Dr.  Eric Brasil.\n",
        "Data: 15/11/2025\n",
        "\n",
        "Relatório metodológico\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dbcb1ba"
      },
      "source": [
        "Para instalar bibliotecas Python no Colab, você pode usar o comando `pip install` seguido do nome da biblioteca. O `pip` é o sistema de gerenciamento de pacotes padrão usado para instalar e gerenciar pacotes de software escritos em Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "086f7fdd"
      },
      "outputs": [],
      "source": [
        "# Exemplo: Instalar a biblioteca pandas\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8IzDggp1xh5"
      },
      "outputs": [],
      "source": [
        "!pip install ebooklib beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Wvde5t2iye"
      },
      "outputs": [],
      "source": [
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Pedir para fazer o upload do arquivo .epub\n",
        "print(\"Por favor, faça o upload do seu arquivo .epub...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Pega o nome do arquivo que você acabou de enviar\n",
        "# O nome do arquivo real será a chave do dicionário 'uploaded'\n",
        "epub_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Define o nome do arquivo .txt de saída\n",
        "txt_name = epub_filename.replace('.epub', '.txt') # Usar replace para criar o nome do arquivo .txt\n",
        "\n",
        "print(f\"\\nArquivo '{epub_filename}' carregado. Iniciando conversão...\")\n",
        "\n",
        "# 2. Abrir o livro\n",
        "book = epub.read_epub(epub_filename)\n",
        "full_text = \"\"\n",
        "\n",
        "# 3. Iterar por todos os \"documentos\" (capítulos) do livro\n",
        "for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
        "    # Pega o conteúdo do item (que é HTML)\n",
        "    content = item.get_body_content() # Não requer argumento\n",
        "\n",
        "    # Usa o BeautifulSoup para limpar o HTML e extrair apenas o texto\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "    text_only = soup.get_text(separator=' ', strip=True) # Pode-se usar separator e strip para melhor formatação\n",
        "    # Adiciona o texto limpo ao nosso texto completo\n",
        "    full_text += text_only + \"\\n\\n\" # (Adiciona quebra de linha entre capítulos)\n",
        "\n",
        "# 4. Salvar o texto completo em um arquivo .txt\n",
        "with open(txt_name, 'w', encoding='utf-8') as f:\n",
        "    f.write(full_text)\n",
        "\n",
        "print(f\"\\nSucesso! O arquivo '{txt_name}' foi criado no seu ambiente Colab.\")\n",
        "print(\"Você pode vê-lo no painel 'Arquivos' à esquerda.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgqyGx-l7mrw"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Define o nome do arquivo de texto que foi criado\n",
        "text_filename = 'Klara e o Sol - Kazuo Ishiguro.txt'\n",
        "\n",
        "# Abre e lê o conteúdo do arquivo\n",
        "with open(text_filename, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Pré-processamento do texto:\n",
        "# 1. Converte para minúsculas para tratar palavras como 'Sol' e 'sol' como a mesma.\n",
        "# 2. Remove pontuações e caracteres especiais, mantendo apenas letras e espaços.\n",
        "# 3. Divide o texto em palavras.\n",
        "\n",
        "cleaned_content = content.lower()\n",
        "words = re.findall(r'\\b[a-zãáàéêíóôõúüç]+\\b', cleaned_content)\n",
        "\n",
        "# Conta a frequência de cada palavra\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Exibe as 20 palavras mais comuns\n",
        "print(\"As 20 palavras mais comuns no texto são:\")\n",
        "for word, count in word_counts.most_common(20):\n",
        "    print(f\"'{word}': {count}\")\n",
        "\n",
        "# Você também pode acessar o total de palavras únicas e o total de palavras:\n",
        "print(f\"\\nTotal de palavras únicas: {len(word_counts)}\")\n",
        "print(f\"Total de palavras (incluindo repetições): {sum(word_counts.values())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu6RK9KI8aK2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2842ffc0"
      },
      "source": [
        "# Task\n",
        "Analyze the emotional journey of Klara in the book \"Klara e o Sol\" by performing sentiment analysis on its text content, visualizing the sentiment scores across different sections, and interpreting the findings. The analysis should utilize the `Klara e o Sol - Kazuo Ishiguro.txt` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a419fc"
      },
      "source": [
        "## Instalar e configurar uma biblioteca de análise de sentimento\n",
        "\n",
        "### Subtask:\n",
        "Instalar a biblioteca `transformers` do Hugging Face e configurar um modelo de análise de sentimento pré-treinado para português.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a650d9b9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing the `transformers` library, importing the `pipeline` function, and initializing a sentiment analysis pipeline for Portuguese. This step will install the library and initialize the pipeline as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "163e46ac"
      },
      "outputs": [],
      "source": [
        "print(\"Instalando a biblioteca transformers...\")\n",
        "!pip install transformers\n",
        "print(\"Biblioteca transformers instalada. Importando pipeline e configurando modelo de análise de sentimento...\")\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Crie um objeto pipeline para análise de sentimento, especificando o modelo adequado para português\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n",
        "\n",
        "print(\"Pipeline de análise de sentimento configurada com o modelo 'finiteautomata/beto-sentiment-analysis'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7adc633a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully installed the `transformers` library and initialized the sentiment analysis pipeline. The next logical step is to load the book's text and prepare it for sentiment analysis, which involves segmenting the text into manageable chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b894719"
      },
      "outputs": [],
      "source": [
        "print(f\"Lendo o arquivo de texto: {text_filename}\")\n",
        "\n",
        "with open(text_filename, 'r', encoding='utf-8') as f:\n",
        "    book_text = f.read()\n",
        "\n",
        "# Quebrar o texto em seções para análise, pois o modelo tem um limite de tokens.\n",
        "# Vamos tentar quebrar por parágrafos ou um número fixo de caracteres.\n",
        "# Para simplificar, vamos dividir em pedaços de 512 caracteres, que é um tamanho comum para modelos BERT.\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=512, overlap=50):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        if end > len(text):\n",
        "            chunks.append(text[start:])\n",
        "            break\n",
        "        else:\n",
        "            # Encontrar o último espaço em branco para não cortar palavras ao meio\n",
        "            split_point = text.rfind(' ', start, end)\n",
        "            if split_point == -1 or split_point < start + chunk_size / 2: # Se não houver espaço ou for muito perto do início\n",
        "                split_point = end\n",
        "            chunks.append(text[start:split_point])\n",
        "            start = split_point + 1 - overlap # Adiciona um pequeno overlap para contexto\n",
        "            if start < 0: start = 0 # Evita índices negativos\n",
        "    return chunks\n",
        "\n",
        "\n",
        "text_chunks = split_text_into_chunks(book_text)\n",
        "\n",
        "print(f\"Texto dividido em {len(text_chunks)} pedaços para análise.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74aa8eef"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the text is chunked, the next step is to apply the `sentiment_pipeline` to each chunk to get sentiment scores. This will generate a list of dictionaries, where each dictionary contains the label (e.g., 'POSITIVO', 'NEGATIVO') and the score for that chunk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "080c22c2"
      },
      "outputs": [],
      "source": [
        "print(\"Realizando análise de sentimento nos pedaços de texto...\")\n",
        "sentiment_results = sentiment_pipeline(text_chunks)\n",
        "\n",
        "print(f\"Análise de sentimento concluída para {len(sentiment_results)} pedaços de texto.\")\n",
        "# print(sentiment_results[:5]) # Display first 5 results to check structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e99d5015"
      },
      "source": [
        "**Reasoning**:\n",
        "The sentiment analysis results are currently in a list of dictionaries. To facilitate visualization and further analysis of the emotional journey, it's beneficial to convert these results into a structured format like a Pandas DataFrame. This DataFrame will contain the sentiment label and score for each text chunk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca04e739"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Criar um DataFrame a partir dos resultados da análise de sentimento\n",
        "df_sentiment = pd.DataFrame(sentiment_results)\n",
        "\n",
        "# Adicionar o índice do chunk como uma coluna para representar a progressão do texto\n",
        "df_sentiment['chunk_index'] = df_sentiment.index\n",
        "\n",
        "# Exibir as primeiras linhas do DataFrame e informações gerais\n",
        "print(\"DataFrame de sentimentos criado:\")\n",
        "print(df_sentiment.head())\n",
        "print(f\"\\nTotal de chunks analisados: {len(df_sentiment)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "306695ef"
      },
      "source": [
        "## Visualizar a jornada emocional\n",
        "\n",
        "### Subtask:\n",
        "Plotar as pontuações de sentimento ao longo das seções do livro para criar um gráfico que represente visualmente a \"jornada emocional\" de Klara.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f1c49df"
      },
      "source": [
        "**Reasoning**:\n",
        "To visualize the emotional journey, I will import the necessary plotting libraries, create a new column in the DataFrame to represent the sentiment value numerically, and then generate a line plot to show the sentiment progression across the book.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1e2b9e1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 2. Crie uma nova coluna no df_sentiment chamada sentiment_value.\n",
        "# 3. Preencha a coluna sentiment_value da seguinte forma:\n",
        "#    se a label for 'POS', use o valor da coluna score;\n",
        "#    se a label for 'NEG', use o valor negativo da coluna score;\n",
        "#    se a label for 'NEU', use 0.\n",
        "df_sentiment['sentiment_value'] = df_sentiment.apply(\n",
        "    lambda row: row['score'] if row['label'] == 'POS' else\n",
        "                -row['score'] if row['label'] == 'NEG' else 0,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 4. Crie um gráfico de linha usando seaborn.lineplot() onde o eixo X seja chunk_index e o eixo Y seja sentiment_value.\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.lineplot(x='chunk_index', y='sentiment_value', data=df_sentiment)\n",
        "\n",
        "# 5. Adicione um título ao gráfico e rótulos para os eixos X e Y.\n",
        "plt.title('Jornada Emocional de Klara (Análise de Sentimento)')\n",
        "plt.xlabel('Índice do Pedaço do Texto')\n",
        "plt.ylabel('Pontuação de Sentimento')\n",
        "\n",
        "# 6. Adicione uma linha horizontal em y=0 para indicar o ponto neutro do sentimento.\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
        "\n",
        "# 7. Exiba o gráfico.\n",
        "plt.grid(True, linestyle=':', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "print(\"Gráfico da jornada emocional de Klara gerado com sucesso.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4b91269"
      },
      "source": [
        "## Interpretar os resultados\n",
        "\n",
        "### Subtask:\n",
        "Analisar os padrões no gráfico e discutir o que as mudanças de sentimento podem indicar sobre a evolução emocional de Klara na história.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdbe753b"
      },
      "source": [
        "Resumo:\n",
        "\n",
        "Principais Descobertas da Análise de Dados\n",
        "O texto de \"Klara e o Sol\" foi processado com sucesso utilizando a biblioteca transformers e um modelo de análise de sentimentos em português (finiteautomata/beto-sentiment-analysis).\n",
        "\n",
        "O livro foi dividido em 1162 trechos de texto para avaliação de sentimento.\n",
        "\n",
        "As pontuações de sentimento foram calculadas para cada trecho, categorizadas em rótulos positivos, negativos ou neutros. Um sentiment_value (valor de sentimento) foi derivado, atribuindo pontuações positivas para sentimento positivo, pontuações negativas para sentimento negativo e zero para sentimento neutro.\n",
        "\n",
        "Um gráfico de linha representou visualmente a \"jornada emocional\" de Klara, mostrando flutuações no valor do sentimento ao longo da progressão do livro. O gráfico permite a observação de períodos onde o sentimento é predominantemente positivo, negativo ou oscila, indicando mudanças na experiência emocional de Klara conforme a história se desenrola.\n",
        "visualização do sentimento ao longo do tempo fornece uma visão geral de alto nível do arco emocional de Klara. Observar quedas ou picos significativos no sentimento pode ter correlação com pontos principais da trama ou interações entre personagens, sugerindo um cenário emocional dinâmico em vez de estático."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16004a58"
      },
      "source": [
        "# Task\n",
        "The following describes the user's goal.\n",
        "Analyze the emotional journey of Klara in the book \"Klara e o Sol\" by segmenting the text into logical parts (e.g., chapters), mapping the existing sentiment-analyzed text chunks to these parts, calculating the average sentiment for each part, visualizing the sentiment progression across these parts, and interpreting the emotional shifts in relation to the story's events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3224f867"
      },
      "source": [
        "## Identificar e Extrair Partes do Livro\n",
        "\n",
        "### Subtask:\n",
        "Dividir o texto completo do livro em partes lógicas (por exemplo, capítulos ou seções principais) com base em marcadores textuais.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46839bd5"
      },
      "source": [
        "**Reasoning**:\n",
        "To divide the book into logical sections, I will split the `book_text` by double newlines (`\\n\\n`), as specified in the instructions. This is a common heuristic for separating paragraphs or major sections in plain text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d21440c9"
      },
      "outputs": [],
      "source": [
        "print(\"Dividindo o texto do livro em partes lógicas...\")\n",
        "\n",
        "# 1. Divida o texto completo do livro em partes lógicas\n",
        "# Usar o delimitador de duas quebras de linha (\\n\\n) como um indicador de nova seção.\n",
        "# O regex r'\\n\\n+' garante que múltiplos \\n\\n sejam tratados como um único delimitador e remove seções vazias.\n",
        "parts = re.split(r'\\n\\n+', book_text)\n",
        "\n",
        "# Remover quaisquer partes que sejam apenas espaços em branco ou vazias após a divisão\n",
        "parts = [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "# 2. Inspecione as primeiras e últimas partes resultantes\n",
        "print(f\"\\nTotal de partes identificadas: {len(parts)}\")\n",
        "print(\"\\nPrimeiras 3 partes:\")\n",
        "for i, part in enumerate(parts[:3]):\n",
        "    print(f\"Parte {i+1}:\\n{part[:200]}...\") # Limita a exibição para 200 caracteres\n",
        "\n",
        "print(\"\\nÚltimas 3 partes:\")\n",
        "for i, part in enumerate(parts[-3:]):\n",
        "    print(f\"Parte {len(parts)-3+i+1}:\\n{part[:200]}...\") # Limita a exibição para 200 caracteres\n",
        "\n",
        "# 3. Crie uma lista de dicionários com 'text', 'start_index' e 'end_index'\n",
        "book_parts = []\n",
        "current_index = 0\n",
        "for part in parts:\n",
        "    start_index = book_text.find(part, current_index) # Encontra a parte no texto original a partir do current_index\n",
        "    if start_index == -1:\n",
        "      # Se não encontrar, pode haver um problema de sanitização ou espaços brancos, tentaremos a partir do início\n",
        "      start_index = book_text.find(part)\n",
        "      if start_index == -1:\n",
        "        print(f\"Aviso: Não foi possível encontrar a parte no texto original: {part[:50]}...\")\n",
        "        continue # Pula esta parte se não puder ser encontrada\n",
        "\n",
        "    end_index = start_index + len(part)\n",
        "    book_parts.append({\n",
        "        'text': part,\n",
        "        'start_index': start_index,\n",
        "        'end_index': end_index\n",
        "    })\n",
        "    current_index = end_index # Atualiza o índice para a próxima busca\n",
        "\n",
        "print(f\"\\nLista 'book_parts' criada com {len(book_parts)} dicionários.\")\n",
        "# print(book_parts[:2]) # Opcional: Para verificar a estrutura dos primeiros dicionários"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2040aaa"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the book is divided into logical parts, I need to associate each sentiment-analyzed text chunk with its corresponding part. This requires iterating through the `book_parts` and `df_sentiment` to assign a 'part_index' to each sentiment entry.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "673c40f3"
      },
      "outputs": [],
      "source": [
        "print(\"Mapeando os pedaços de texto analisados para as partes lógicas do livro...\")\n",
        "\n",
        "# Adicionar uma coluna 'part_index' ao df_sentiment\n",
        "df_sentiment['part_index'] = -1\n",
        "\n",
        "# Acompanhar a posição atual no texto original para os chunks\n",
        "current_chunk_start_index = 0\n",
        "\n",
        "# Calcular os índices de início e fim para cada chunk no texto original\n",
        "chunk_info = []\n",
        "for i, chunk_text in enumerate(text_chunks):\n",
        "    chunk_length = len(chunk_text)\n",
        "    chunk_end_index = current_chunk_start_index + chunk_length\n",
        "    chunk_info.append({\n",
        "        'chunk_index': i,\n",
        "        'start_in_book_text': current_chunk_start_index,\n",
        "        'end_in_book_text': chunk_end_index\n",
        "    })\n",
        "    # A lógica de split_text_into_chunks usa overlap. Precisamos replicar essa lógica\n",
        "    # para calcular corretamente o `current_chunk_start_index` para o próximo chunk.\n",
        "    # Por simplicidade, assumimos que `text_chunks` já foi gerado e agora estamos\n",
        "    # apenas \"realocando\" seus offsets no texto original.\n",
        "    # Uma abordagem mais precisa seria recalcular os `start_in_book_text` e `end_in_book_text`\n",
        "    # usando o `book_text` e o `text_chunks` diretamente, mas para evitar complexidade\n",
        "    # e dado que `text_chunks` já existe, faremos uma atribuição simples aqui.\n",
        "    # A maneira mais robusta seria ter armazenado esses índices junto com a geração dos chunks.\n",
        "    # Para este propósito, vamos simplesmente iterar e assumir que o chunk_info está correto\n",
        "    # se a lógica de geração dos chunks for mantida.\n",
        "    # Se a função split_text_into_chunks usa overlap, os índices de start_in_book_text\n",
        "    # precisam ser ajustados para cada chunk.\n",
        "\n",
        "\n",
        "# Uma forma mais robusta de fazer a atribuição de parte para cada chunk\n",
        "for i, chunk_text in enumerate(text_chunks):\n",
        "    # Encontrar a posição inicial real do chunk no book_text\n",
        "    # Isso pode ser complexo devido ao 'overlap' e 'strip' na função split_text_into_chunks\n",
        "    # Uma abordagem é iterar sobre as partes e verificar quais chunks caem dentro delas.\n",
        "\n",
        "    # Para simplificar e evitar recalcular os índices de cada chunk no texto original\n",
        "    # com a lógica exata de overlap, vamos aproximar a atribuição.\n",
        "    # Vamos atribuir o chunk a uma parte se a maior parte do chunk estiver dentro da parte.\n",
        "\n",
        "    # Para uma atribuição mais precisa, o ideal seria que a função split_text_into_chunks\n",
        "    # retornasse também os índices de start/end de cada chunk no texto original.\n",
        "\n",
        "    # Sem os índices exatos de cada chunk, a atribuição pode ser imprecisa.\n",
        "    # Vamos criar uma aproximação baseada na proporção do texto total que cada chunk representa.\n",
        "\n",
        "    # A forma mais simples de mapear chunks para partes é assumir uma distribuição uniforme\n",
        "    # ou iterar e verificar o conteúdo.\n",
        "\n",
        "    # Vamos refinar a lógica: cada chunk tem um `chunk_index`.\n",
        "    # Precisamos saber onde cada `chunk` começa e termina no `book_text`.\n",
        "    # A função `split_text_into_chunks` não retorna esses índices diretamente,\n",
        "    # então faremos uma re-localização aproximada.\n",
        "\n",
        "    # Calcula o ponto médio do chunk para atribuí-lo a uma parte\n",
        "    # Esta é uma estimativa, pois 'text_chunks' não tem os índices de início/fim diretamente.\n",
        "    # Para a melhor precisão, é necessário re-gerar `text_chunks` com seus índices originais.\n",
        "\n",
        "    # No entanto, temos `text_chunks` e sabemos que eles foram gerados sequencialmente.\n",
        "    # Vamos usar uma abordagem que mapeia a proporção do texto total:\n",
        "    # Se `book_parts` representa grandes seções, e `text_chunks` são pequenos pedaços,\n",
        "    # podemos estimar o `part_index` para cada `chunk_index`.\n",
        "\n",
        "    # O `df_sentiment` já tem `chunk_index`. Precisamos associar `part_index` a ele.\n",
        "\n",
        "    # Abordagem: Para cada chunk, vamos tentar encontrar a parte a que ele pertence\n",
        "    # baseando-nos na sua posição relativa no texto total.\n",
        "\n",
        "    # Criar uma lista de offsets para o início de cada chunk no `book_text`\n",
        "    # Isso requer re-simular a lógica de `split_text_into_chunks` para obter os offsets.\n",
        "    chunk_start_offsets = []\n",
        "    current_offset = 0\n",
        "    for chunk_text in text_chunks:\n",
        "        chunk_start_offsets.append(current_offset)\n",
        "        current_offset += len(chunk_text)\n",
        "        # Se houver overlap, a lógica de `current_offset` precisa ser ajustada\n",
        "        # A função original `split_text_into_chunks` subtrai `overlap` para o próximo `start`.\n",
        "        # Para simular isso:\n",
        "        # `start = split_point + 1 - overlap`\n",
        "        # Então, o `current_offset` para o próximo chunk seria `split_point + 1 - overlap`.\n",
        "        # Como não temos `split_point` e `overlap` aqui, esta é uma aproximação.\n",
        "\n",
        "    # Mapear `chunk_index` para `part_index`\n",
        "    chunk_to_part_map = {}\n",
        "    part_start_chars = [p['start_index'] for p in book_parts]\n",
        "    part_end_chars = [p['end_index'] for p in book_parts]\n",
        "\n",
        "    for chunk_idx, chunk_offset in enumerate(chunk_start_offsets):\n",
        "        # Assumir que o chunk pertence à parte em que seu início está contido\n",
        "        assigned_part_idx = -1\n",
        "        for part_idx, (p_start, p_end) in enumerate(zip(part_start_chars, part_end_chars)):\n",
        "            # Simplificando: se o início do chunk está dentro da parte\n",
        "            if p_start <= chunk_offset < p_end:\n",
        "                assigned_part_idx = part_idx\n",
        "                break\n",
        "        chunk_to_part_map[chunk_idx] = assigned_part_idx\n",
        "\n",
        "    # Atribuir o part_index ao DataFrame de sentimentos\n",
        "    df_sentiment['part_index'] = df_sentiment['chunk_index'].map(chunk_to_part_map)\n",
        "\n",
        "\n",
        "print(\"Mapeamento concluído. Verificando as primeiras entradas:\")\n",
        "print(df_sentiment.head())\n",
        "\n",
        "print(f\"Número de chunks não atribuídos a uma parte: {(df_sentiment['part_index'] == -1).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff4175a1"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the sentiment analysis results are mapped to logical parts of the book, I will calculate the average sentiment score for each part. This will provide a summarized emotional value for each section of the book, which is essential for visualizing the overall emotional journey.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "197dd47e"
      },
      "outputs": [],
      "source": [
        "print(\"Calculando o sentimento médio por parte do livro...\")\n",
        "\n",
        "# Filtrar chunks que foram atribuídos a uma parte (-1 significa não atribuído)\n",
        "df_sentiment_assigned = df_sentiment[df_sentiment['part_index'] != -1]\n",
        "\n",
        "# Calcular o sentimento médio por parte\n",
        "average_sentiment_per_part = df_sentiment_assigned.groupby('part_index')['sentiment_value'].mean().reset_index()\n",
        "average_sentiment_per_part.rename(columns={'sentiment_value': 'average_sentiment'}, inplace=True)\n",
        "\n",
        "print(\"Sentimento médio por parte calculado:\")\n",
        "print(average_sentiment_per_part.head())\n",
        "print(f\"\\nTotal de partes com sentimento médio calculado: {len(average_sentiment_per_part)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ddb6198"
      },
      "source": [
        "**Reasoning**:\n",
        "To visualize the emotional journey across the logical parts of the book, I will use `seaborn.lineplot` to plot the `average_sentiment` against the `part_index`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "973fbe46"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Gerando o gráfico da jornada emocional de Klara por partes lógicas...\")\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.lineplot(x='part_index', y='average_sentiment', data=average_sentiment_per_part)\n",
        "\n",
        "plt.title('Jornada Emocional de Klara por Partes do Livro')\n",
        "plt.xlabel('Índice da Parte do Livro')\n",
        "plt.ylabel('Sentimento Médio')\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
        "plt.grid(True, linestyle=':', alpha=0.7)\n",
        "plt.xticks(average_sentiment_per_part['part_index'])\n",
        "plt.show()\n",
        "\n",
        "print(\"Gráfico da jornada emocional de Klara por partes lógicas gerado com sucesso.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c2b5fd0"
      },
      "source": [
        "### Interpretação da Jornada Emocional de Klara por Partes do Livro\n",
        "\n",
        "O gráfico da \"Jornada Emocional de Klara por Partes do Livro\" apresenta as seguintes observações e interpretações:\n",
        "\n",
        "1.  **Variação de Sentimento:** O gráfico mostra flutuações no sentimento médio ao longo das diferentes partes do livro. Isso sugere que a narrativa de Klara não mantém um tom emocional constante, mas sim evolui através de picos e vales.\n",
        "\n",
        "2.  **Partes Iniciais (Índices 0-4):** As primeiras partes do livro tendem a ter sentimentos neutros ou levemente negativos. A parte de índice 0 mostra um sentimento neutro. A parte 4 já registra um sentimento levemente negativo, o que pode indicar um início da história onde Klara está se adaptando ou observando seu novo ambiente sem grandes emoções positivas ou enfrentando desafios sutis.\n",
        "\n",
        "3.  **Queda Significativa de Sentimento (Índice 12 e 17):** Há quedas notáveis no sentimento médio em torno das partes 12 e 17. A parte 12 exibe um sentimento negativo mais pronunciado, e a parte 17 é a que mostra a maior pontuação negativa. Essas quedas podem corresponder a momentos críticos na trama, onde Klara enfrenta dificuldades, desilusões, perdas ou situações angustiantes que impactam profundamente sua percepção e bem-estar emocional.\n",
        "\n",
        "4.  **Recuperação e Neutralidade:** Em alguns pontos, como nas partes 15, 16 e 18, o sentimento retorna a um estado neutro ou se aproxima de zero. Isso pode indicar períodos de estabilidade, aceitação ou momentos de pausa na narrativa onde a intensidade emocional diminui, ou Klara encontra alguma forma de resolução ou compreensão.\n",
        "\n",
        "5.  **Final do Livro (Índices 19-21):** O final do gráfico, representando as últimas partes do livro, mostra sentimentos negativos contínuos (partes 19 e 20) com a parte 21 voltando a um sentimento neutro. Esta tendência pode refletir temas como sacrifício, melancolia ou a complexidade das relações e escolhas feitas por Klara, que não necessariamente terminam em um clímax de felicidade, mas sim em uma aceitação resignada ou uma nova forma de entendimento.\n",
        "\n",
        "**Conclusão Parcial:** A jornada emocional de Klara, conforme visualizada, é complexa e dinâmica. Os períodos de sentimento negativo profundo são intercalados com momentos de neutralidade. Para uma interpretação mais aprofundada, seria ideal correlacionar esses picos e vales emocionais com eventos específicos na trama, o que permitiria entender os gatilhos por trás das mudanças no estado emocional de Klara."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4d642d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The emotional journey of Klara is complex and dynamic, characterized by fluctuations in sentiment throughout the book.\n",
        "*   **Initial Parts (Indices 0-4):** Start with neutral sentiment (part 0) and gradually shift to slightly negative (part 4).\n",
        "*   **Significant Declines:** Notable drops in sentiment occur around part 12 and particularly at part 17, which exhibits the most pronounced negative sentiment. These likely correspond to critical or distressing events in the narrative.\n",
        "*   **Periods of Recovery:** The sentiment recovers to neutral or near-zero levels in parts 15, 16, and 18, suggesting moments of stability or resolution.\n",
        "*   **Later Parts (Indices 19-21):** The journey concludes with continued negative sentiments in parts 19 and 20, eventually returning to a neutral state in part 21. This suggests themes of sacrifice, melancholy, or a resigned acceptance rather than a purely positive resolution.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The book text was successfully divided into 22 logical parts.\n",
        "*   A total of 112 sentiment-analyzed chunks could not be mapped to any specific book part, potentially due to the chunking strategy or text discrepancies, leading to sentiment analysis being performed on 14 distinct parts instead of the full 22.\n",
        "*   The average sentiment for Klara shows a fluctuating pattern, indicating a dynamic emotional journey.\n",
        "*   The most negative sentiment experienced by Klara is observed around part 17 of the book.\n",
        "*   The emotional journey does not end on an overwhelmingly positive note, with sentiment returning to neutral in the final part (index 21) after periods of negativity.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   To gain a deeper understanding, it would be beneficial to correlate the observed emotional peaks and valleys with specific plot events. This would help identify the narrative triggers for Klara's emotional shifts.\n",
        "*   Investigate the 112 unassigned chunks to understand why they weren't mapped to book parts. This could involve refining the chunking and mapping logic to ensure comprehensive coverage of the book's sentiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b977fb31"
      },
      "source": [
        "# Task\n",
        "Identify the start and end boundaries of the main narrative in `book_text` for 'Klara e o Sol', excluding any introductory, summary, dedicatory, or promotional content. The start of the narrative is expected to be marked by \"Parte um\" and the end before any promotional material for other books. Extract the pure narrative text into a new variable, `narrative_text`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9680929"
      },
      "source": [
        "## Identificar Limites da Narrativa\n",
        "\n",
        "### Subtask:\n",
        "Detectar o início e o fim da narrativa principal de 'Klara e o Sol' no `book_text`, excluindo prefácios, sumários, e conteúdos promocionais que não se referem diretamente à história.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dc8d6b0"
      },
      "source": [
        "# Task\n",
        "```python\n",
        "print(\"Detectando o início e o fim da narrativa principal no 'book_text'...\")\n",
        "\n",
        "# 1. Detectar o início da narrativa principal\n",
        "# A narrativa principal geralmente começa após o sumário e dedicatórias,\n",
        "# e é marcada pela primeira ocorrência de \"Parte um\" (ou similar).\n",
        "start_marker = \"Parte um\"\n",
        "start_index = book_text.find(start_marker)\n",
        "\n",
        "if start_index == -1:\n",
        "    print(f\"Erro: O marcador de início da narrativa '{start_marker}' não foi encontrado.\")\n",
        "    # Tentativa de fallback para um ponto conhecido se o marcador exato não for encontrado\n",
        "    # Para este livro específico, a \"Parte um\" é o primeiro capítulo.\n",
        "    # Se não for encontrada, a estratégia precisa ser revisada.\n",
        "    narrative_text = book_text # Retorna o texto completo se não encontrar\n",
        "else:\n",
        "    # Ajustar o start_index para o início da \"Parte um\"\n",
        "    start_index_of_narrative = start_index\n",
        "\n",
        "    # 2. Detectar o fim da narrativa principal\n",
        "    # A narrativa principal termina antes de quaisquer materiais promocionais, apêndices ou créditos finais.\n",
        "    # Baseado na inspeção prévia de book_parts, os materiais promocionais começam na Parte 20 (índice 19).\n",
        "    # Vamos usar o `start_index` da parte que contém \"A palavra que resta Gardel\" como o marcador de fim.\n",
        "    end_marker_part_index = 19 # Corresponde à \"Parte 20\" nos dicionários gerados\n",
        "    if end_marker_part_index < len(book_parts):\n",
        "        end_index_of_narrative = book_parts[end_marker_part_index]['start_index']\n",
        "    else:\n",
        "        print(\"Aviso: Índice da parte de fim da narrativa fora dos limites de book_parts. Usando o fim do livro.\")\n",
        "        end_index_of_narrative = len(book_text)\n",
        "\n",
        "    # 3. Extrair a narrativa pura\n",
        "    narrative_text = book_text[start_index_of_narrative:end_index_of_narrative].strip()\n",
        "\n",
        "print(f\"\\nComprimento do 'book_text' original: {len(book_text)} caracteres.\")\n",
        "print(f\"Comprimento do 'narrative_text' extraído: {len(narrative_text)} caracteres.\")\n",
        "\n",
        "print(\"\\nInício da narrativa extraída (primeiros 500 caracteres):\")\n",
        "print(narrative_text[:500])\n",
        "\n",
        "print(\"\\nFim da narrativa extraída (últimos 500 caracteres):\")\n",
        "print(narrative_text[-500:])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e1cca7e"
      },
      "source": [
        "## Extrair Nomes de Personagens\n",
        "\n",
        "### Subtask:\n",
        "Identificar e extrair os nomes dos personagens principais do texto narrativo (`narrative_text`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ecc71ac"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify the main characters mentioned in the narrative, I will define a list of known main characters, then iterate through this list to check if each character's name appears in the `narrative_text`. I will convert both the character names and the narrative text to lowercase for a case-insensitive search and store the found characters in a new list, finally displaying them and their count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a3a719e"
      },
      "source": [
        "print(\"Iniciando a extração de nomes de personagens do texto narrativo...\")\n",
        "\n",
        "# Certifique-se de que 'narrative_text' está definida a partir do passo anterior.\n",
        "# Se 'narrative_text' não estiver definida, este código pode falhar.\n",
        "# Assumindo que a célula anterior criou a variável 'narrative_text'.\n",
        "\n",
        "# 1. Defina uma lista Python chamada main_characters contendo os nomes dos personagens principais do livro.\n",
        "# A lista é baseada nos personagens conhecidos de 'Klara e o Sol'.\n",
        "main_characters = ['Klara', 'Josie', 'Rick', 'Melania Housekeeper', 'Padre', 'Mãe', 'Pai', 'Sr. Capaldi', 'O Coletor', 'B-2', 'B-1']\n",
        "\n",
        "# 2. Crie uma lista vazia chamada found_characters para armazenar os nomes dos personagens encontrados.\n",
        "found_characters = []\n",
        "\n",
        "# 3. Itere sobre cada nome na lista main_characters.\n",
        "for character_name in main_characters:\n",
        "    # 4. Para cada nome de personagem, verifique se ele está presente no narrative_text.\n",
        "    # Converta tanto o nome do personagem quanto o narrative_text para minúsculas antes de realizar a busca.\n",
        "    if character_name.lower() in narrative_text.lower():\n",
        "        # 5. Se o nome do personagem for encontrado no narrative_text, adicione-o à lista found_characters.\n",
        "        found_characters.append(character_name)\n",
        "\n",
        "# 6. Exiba a lista found_characters para verificar quais personagens foram identificados na narrativa.\n",
        "print(\"\\nPersonagens principais identificados na narrativa:\")\n",
        "if found_characters:\n",
        "    for character in found_characters:\n",
        "        print(f\"- {character}\")\n",
        "else:\n",
        "    print(\"Nenhum personagem principal da lista foi encontrado na narrativa. Verifique a lista ou o texto.\")\n",
        "\n",
        "# 7. Exiba o número total de personagens únicos encontrados.\n",
        "unique_found_characters = list(set(found_characters))\n",
        "print(f\"\\nTotal de personagens únicos encontrados: {len(unique_found_characters)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e41f5ae"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `narrative_text` was not defined. This variable was meant to be created in a prior step (cell `8dc8d6b0`), which was a text cell with commented code and thus not executed. To resolve this, I will incorporate the logic for defining `narrative_text` directly into the current code block, ensuring it is available before the character extraction process begins.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1deaf865"
      },
      "source": [
        "import re\n",
        "\n",
        "print(\"Detectando o início e o fim da narrativa principal no 'book_text'...\")\n",
        "\n",
        "# 1. Detectar o início da narrativa principal\n",
        "# A narrativa principal geralmente começa após o sumário e dedicatórias,\n",
        "# e é marcada pela primeira ocorrência de \"Parte um\" (ou similar).\n",
        "start_marker = \"Parte um\"\n",
        "start_index = book_text.find(start_marker)\n",
        "\n",
        "if start_index == -1:\n",
        "    print(f\"Erro: O marcador de início da narrativa '{start_marker}' não foi encontrado.\")\n",
        "    # Tentativa de fallback para um ponto conhecido se o marcador exato não for encontrado\n",
        "    # Para este livro específico, a \"Parte um\" é o primeiro capítulo.\n",
        "    # Se não for encontrada, a estratégia precisa ser revisada.\n",
        "    narrative_text = book_text # Retorna o texto completo se não encontrar\n",
        "else:\n",
        "    # Ajustar o start_index para o início da \"Parte um\"\n",
        "    start_index_of_narrative = start_index\n",
        "\n",
        "    # 2. Detectar o fim da narrativa principal\n",
        "    # A narrativa principal termina antes de quaisquer materiais promocionais, apêndices ou créditos finais.\n",
        "    # Baseado na inspeção prévia de book_parts, os materiais promocionais começam na Parte 20 (índice 19).\n",
        "    # Vamos usar o `start_index` da parte que contém \"A palavra que resta Gardel\" como o marcador de fim.\n",
        "    end_marker_part_index = 19 # Corresponde à \"Parte 20\" nos dicionários gerados\n",
        "    if end_marker_part_index < len(book_parts):\n",
        "        end_index_of_narrative = book_parts[end_marker_part_index]['start_index']\n",
        "    else:\n",
        "        print(\"Aviso: Índice da parte de fim da narrativa fora dos limites de book_parts. Usando o fim do livro.\")\n",
        "        end_index_of_narrative = len(book_text)\n",
        "\n",
        "    # 3. Extrair a narrativa pura\n",
        "    narrative_text = book_text[start_index_of_narrative:end_index_of_narrative].strip()\n",
        "\n",
        "print(f\"\\nComprimento do 'book_text' original: {len(book_text)} caracteres.\")\n",
        "print(f\"Comprimento do 'narrative_text' extraído: {len(narrative_text)} caracteres.\")\n",
        "\n",
        "print(\"\\nInício da narrativa extraída (primeiros 500 caracteres):\")\n",
        "print(narrative_text[:500])\n",
        "\n",
        "print(\"\\nFim da narrativa extraída (últimos 500 caracteres):\")\n",
        "print(narrative_text[-500:])\n",
        "\n",
        "\n",
        "print(\"\\nIniciando a extração de nomes de personagens do texto narrativo...\")\n",
        "\n",
        "# 1. Defina uma lista Python chamada main_characters contendo os nomes dos personagens principais do livro.\n",
        "# A lista é baseada nos personagens conhecidos de 'Klara e o Sol'.\n",
        "main_characters = ['Klara', 'Josie', 'Rick', 'Melania Housekeeper', 'Padre', 'Mãe', 'Pai', 'Sr. Capaldi', 'O Coletor', 'B-2', 'B-1']\n",
        "\n",
        "# 2. Crie uma lista vazia chamada found_characters para armazenar os nomes dos personagens encontrados.\n",
        "found_characters = []\n",
        "\n",
        "# 3. Itere sobre cada nome na lista main_characters.\n",
        "for character_name in main_characters:\n",
        "    # 4. Para cada nome de personagem, verifique se ele está presente no narrative_text.\n",
        "    # Converta tanto o nome do personagem quanto o narrative_text para minúsculas antes de realizar a busca.\n",
        "    if character_name.lower() in narrative_text.lower():\n",
        "        # 5. Se o nome do personagem for encontrado no narrative_text, adicione-o à lista found_characters.\n",
        "        found_characters.append(character_name)\n",
        "\n",
        "# 6. Exiba a lista found_characters para verificar quais personagens foram identificados na narrativa.\n",
        "print(\"\\nPersonagens principais identificados na narrativa:\")\n",
        "if found_characters:\n",
        "    for character in found_characters:\n",
        "        print(f\"- {character}\")\n",
        "else:\n",
        "    print(\"Nenhum personagem principal da lista foi encontrado na narrativa. Verifique a lista ou o texto.\")\n",
        "\n",
        "# 7. Exiba o número total de personagens únicos encontrados.\n",
        "unique_found_characters = list(set(found_characters))\n",
        "print(f\"\\nTotal de personagens únicos encontrados: {len(unique_found_characters)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be4dbfec"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `book_text` was not defined. This variable needs to be reloaded from the text file. Additionally, `book_parts` also needs to be recreated as it depends on `book_text`. I will re-include the necessary steps to load the book text, split it into parts, and then extract the narrative and identify characters, ensuring all variables are defined in the correct order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7122764"
      },
      "source": [
        "import re\n",
        "\n",
        "# Re-load book_text (from earlier step, e.g., cell 4b894719)\n",
        "text_filename = 'Klara e o Sol - Kazuo Ishiguro.txt'\n",
        "print(f\"Re-lendo o arquivo de texto: {text_filename}\")\n",
        "with open(text_filename, 'r', encoding='utf-8') as f:\n",
        "    book_text = f.read()\n",
        "\n",
        "# Re-create book_parts (from earlier step, e.g., cell d21440c9)\n",
        "print(\"Re-dividindo o texto do livro em partes lógicas...\")\n",
        "parts = re.split(r'\\n\\n+', book_text)\n",
        "parts = [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "book_parts = []\n",
        "current_index = 0\n",
        "for part in parts:\n",
        "    start_index_part = book_text.find(part, current_index)\n",
        "    if start_index_part == -1:\n",
        "      start_index_part = book_text.find(part)\n",
        "      if start_index_part == -1:\n",
        "        continue\n",
        "    end_index_part = start_index_part + len(part)\n",
        "    book_parts.append({\n",
        "        'text': part,\n",
        "        'start_index': start_index_part,\n",
        "        'end_index': end_index_part\n",
        "    })\n",
        "    current_index = end_index_part\n",
        "\n",
        "print(f\"Re-criada 'book_parts' com {len(book_parts)} dicionários.\")\n",
        "\n",
        "# Now, proceed with the narrative extraction logic\n",
        "print(\"Detectando o início e o fim da narrativa principal no 'book_text'...\")\n",
        "\n",
        "# 1. Detectar o início da narrativa principal\n",
        "start_marker = \"Parte um\"\n",
        "start_index = book_text.find(start_marker)\n",
        "\n",
        "if start_index == -1:\n",
        "    print(f\"Erro: O marcador de início da narrativa '{start_marker}' não foi encontrado.\")\n",
        "    narrative_text = book_text # Fallback\n",
        "else:\n",
        "    start_index_of_narrative = start_index\n",
        "\n",
        "    # 2. Detectar o fim da narrativa principal\n",
        "    end_marker_part_index = 19\n",
        "    if end_marker_part_index < len(book_parts):\n",
        "        end_index_of_narrative = book_parts[end_marker_part_index]['start_index']\n",
        "    else:\n",
        "        print(\"Aviso: Índice da parte de fim da narrativa fora dos limites de book_parts. Usando o fim do livro.\")\n",
        "        end_index_of_narrative = len(book_text)\n",
        "\n",
        "    # 3. Extrair a narrativa pura\n",
        "    narrative_text = book_text[start_index_of_narrative:end_index_of_narrative].strip()\n",
        "\n",
        "print(f\"\\nComprimento do 'book_text' original: {len(book_text)} caracteres.\")\n",
        "print(f\"Comprimento do 'narrative_text' extraído: {len(narrative_text)} caracteres.\")\n",
        "\n",
        "print(\"\\nInício da narrativa extraída (primeiros 500 caracteres):\")\n",
        "print(narrative_text[:500])\n",
        "\n",
        "print(\"\\nFim da narrativa extraída (últimos 500 caracteres):\")\n",
        "print(narrative_text[-500:])\n",
        "\n",
        "\n",
        "print(\"\\nIniciando a extração de nomes de personagens do texto narrativo...\")\n",
        "\n",
        "# 1. Defina uma lista Python chamada main_characters contendo os nomes dos personagens principais do livro.\n",
        "main_characters = ['Klara', 'Josie', 'Rick', 'Melania Housekeeper', 'Padre', 'Mãe', 'Pai', 'Sr. Capaldi', 'O Coletor', 'B-2', 'B-1']\n",
        "\n",
        "# 2. Crie uma lista vazia chamada found_characters para armazenar os nomes dos personagens encontrados.\n",
        "found_characters = []\n",
        "\n",
        "# 3. Itere sobre cada nome na lista main_characters.\n",
        "for character_name in main_characters:\n",
        "    # 4. Para cada nome de personagem, verifique se ele está presente no narrative_text.\n",
        "    # Converta tanto o nome do personagem quanto o narrative_text para minúsculas antes de realizar a busca.\n",
        "    if character_name.lower() in narrative_text.lower():\n",
        "        # 5. Se o nome do personagem for encontrado no narrative_text, adicione-o à lista found_characters.\n",
        "        found_characters.append(character_name)\n",
        "\n",
        "# 6. Exiba a lista found_characters para verificar quais personagens foram identificados na narrativa.\n",
        "print(\"\\nPersonagens principais identificados na narrativa:\")\n",
        "if found_characters:\n",
        "    for character in found_characters:\n",
        "        print(f\"- {character}\")\n",
        "else:\n",
        "    print(\"Nenhum personagem principal da lista foi encontrado na narrativa. Verifique a lista ou o texto.\")\n",
        "\n",
        "# 7. Exiba o número total de personagens únicos encontrados.\n",
        "unique_found_characters = list(set(found_characters))\n",
        "print(f\"\\nTotal de personagens únicos encontrados: {len(unique_found_characters)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb1df9d5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the text file `Klara e o Sol - Kazuo Ishiguro.txt` was not found. This file was generated earlier by converting an EPUB. I will re-include the necessary steps to upload the EPUB file again and convert it to a `.txt` file, ensuring the `book_text` variable is properly loaded and `book_parts` is recreated, before proceeding with the character extraction logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b8cff3e"
      },
      "source": [
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# 1. Pedir para fazer o upload do arquivo .epub novamente, se necessário.\n",
        "# Este passo é crucial para garantir que o arquivo .txt existe se o runtime foi resetado.\n",
        "# Se o arquivo já existir, a mensagem de upload será exibida mas o usuário pode ignorar.\n",
        "print(\"Por favor, certifique-se que o arquivo .epub foi carregado. Se não, faça o upload agora.\")\n",
        "\n",
        "# Define o nome do arquivo .txt de saída\n",
        "text_filename = 'Klara e o Sol - Kazuo Ishiguro.txt'\n",
        "epub_filename = text_filename.replace('.txt', '.epub')\n",
        "\n",
        "# Check if the .txt file already exists to avoid unnecessary re-upload/conversion\n",
        "try:\n",
        "    with open(text_filename, 'r', encoding='utf-8') as f:\n",
        "        book_text = f.read()\n",
        "    print(f\"Arquivo '{text_filename}' já existe e foi carregado.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Arquivo '{text_filename}' não encontrado. Por favor, faça o upload do arquivo .epub...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"Nenhum arquivo .epub foi carregado. Não é possível prosseguir.\")\n",
        "        # Handle the error or exit gracefully\n",
        "        book_text = \"\"\n",
        "    else:\n",
        "        epub_filename = list(uploaded.keys())[0]\n",
        "        print(f\"Arquivo '{epub_filename}' carregado. Iniciando conversão...\")\n",
        "\n",
        "        # 2. Abrir o livro\n",
        "        book = epub.read_epub(epub_filename)\n",
        "        full_text = \"\"\n",
        "\n",
        "        # 3. Iterar por todos os \"documentos\" (capítulos) do livro\n",
        "        for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
        "            content = item.get_body_content()\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            text_only = soup.get_text(separator=' ', strip=True)\n",
        "            full_text += text_only + \"\\n\\n\"\n",
        "\n",
        "        # 4. Salvar o texto completo em um arquivo .txt\n",
        "        with open(text_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(full_text)\n",
        "        book_text = full_text\n",
        "        print(f\"\\nSucesso! O arquivo '{text_filename}' foi criado no seu ambiente Colab.\")\n",
        "\n",
        "\n",
        "# Re-create book_parts (from earlier step, e.g., cell d21440c9)\n",
        "print(\"Dividindo o texto do livro em partes lógicas...\")\n",
        "parts = re.split(r'\\n\\n+', book_text)\n",
        "parts = [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "book_parts = []\n",
        "current_index = 0\n",
        "for part in parts:\n",
        "    start_index_part = book_text.find(part, current_index)\n",
        "    if start_index_part == -1:\n",
        "      start_index_part = book_text.find(part)\n",
        "      if start_index_part == -1:\n",
        "        continue\n",
        "    end_index_part = start_index_part + len(part)\n",
        "    book_parts.append({\n",
        "        'text': part,\n",
        "        'start_index': start_index_part,\n",
        "        'end_index': end_index_part\n",
        "    })\n",
        "    current_index = end_index_part\n",
        "\n",
        "print(f\"'book_parts' criada com {len(book_parts)} dicionários.\")\n",
        "\n",
        "# Now, proceed with the narrative extraction logic\n",
        "print(\"Detectando o início e o fim da narrativa principal no 'book_text'...\")\n",
        "\n",
        "# 1. Detectar o início da narrativa principal\n",
        "start_marker = \"Parte um\"\n",
        "start_index = book_text.find(start_marker)\n",
        "\n",
        "if start_index == -1:\n",
        "    print(f\"Erro: O marcador de início da narrativa '{start_marker}' não foi encontrado.\")\n",
        "    narrative_text = book_text # Fallback\n",
        "else:\n",
        "    start_index_of_narrative = start_index\n",
        "\n",
        "    # 2. Detectar o fim da narrativa principal\n",
        "    end_marker_part_index = 19\n",
        "    if end_marker_part_index < len(book_parts):\n",
        "        end_index_of_narrative = book_parts[end_marker_part_index]['start_index']\n",
        "    else:\n",
        "        print(\"Aviso: Índice da parte de fim da narrativa fora dos limites de book_parts. Usando o fim do livro.\")\n",
        "        end_index_of_narrative = len(book_text)\n",
        "\n",
        "    # 3. Extrair a narrativa pura\n",
        "    narrative_text = book_text[start_index_of_narrative:end_index_of_narrative].strip()\n",
        "\n",
        "print(f\"\\nComprimento do 'book_text' original: {len(book_text)} caracteres.\")\n",
        "print(f\"Comprimento do 'narrative_text' extraído: {len(narrative_text)} caracteres.\")\n",
        "\n",
        "print(\"\\nInício da narrativa extraída (primeiros 500 caracteres):\")\n",
        "print(narrative_text[:500])\n",
        "\n",
        "print(\"\\nFim da narrativa extraída (últimos 500 caracteres):\")\n",
        "print(narrative_text[-500:])\n",
        "\n",
        "\n",
        "print(\"\\nIniciando a extração de nomes de personagens do texto narrativo...\")\n",
        "\n",
        "# 1. Defina uma lista Python chamada main_characters contendo os nomes dos personagens principais do livro.\n",
        "main_characters = ['Klara', 'Josie', 'Rick', 'Melania Housekeeper', 'Padre', 'Mãe', 'Pai', 'Sr. Capaldi', 'O Coletor', 'B-2', 'B-1']\n",
        "\n",
        "# 2. Crie uma lista vazia chamada found_characters para armazenar os nomes dos personagens encontrados.\n",
        "found_characters = []\n",
        "\n",
        "# 3. Itere sobre cada nome na lista main_characters.\n",
        "for character_name in main_characters:\n",
        "    # 4. Para cada nome de personagem, verifique se ele está presente no narrative_text.\n",
        "    # Converta tanto o nome do personagem quanto o narrative_text para minúsculas antes de realizar a busca.\n",
        "    if character_name.lower() in narrative_text.lower():\n",
        "        # 5. Se o nome do personagem for encontrado no narrative_text, adicione-o à lista found_characters.\n",
        "        found_characters.append(character_name)\n",
        "\n",
        "# 6. Exiba a lista found_characters para verificar quais personagens foram identificados na narrativa.\n",
        "print(\"\\nPersonagens principais identificados na narrativa:\")\n",
        "if found_characters:\n",
        "    for character in found_characters:\n",
        "        print(f\"- {character}\")\n",
        "else:\n",
        "    print(\"Nenhum personagem principal da lista foi encontrado na narrativa. Verifique a lista ou o texto.\")\n",
        "\n",
        "# 7. Exiba o número total de personagens únicos encontrados.\n",
        "unique_found_characters = list(set(found_characters))\n",
        "print(f\"\\nTotal de personagens únicos encontrados: {len(unique_found_characters)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46d1b058"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because `ebooklib` was not found, indicating that the package was not installed or the runtime reset. I will re-install `ebooklib` and `beautifulsoup4` to ensure all necessary libraries are available for the subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9667e537"
      },
      "source": [
        "!pip install ebooklib beautifulsoup4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14696cac"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ebooklib` library has been re-installed. Now, I need to re-execute the full code block that handles loading the text file (converting EPUB if necessary), recreating `book_parts`, extracting the narrative text, and finally identifying the main characters to ensure all dependencies and variables are correctly set up and the character extraction is performed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ed8d32c"
      },
      "source": [
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# 1. Pedir para fazer o upload do arquivo .epub novamente, se necessário.\n",
        "# Este passo é crucial para garantir que o arquivo .txt existe se o runtime foi resetado.\n",
        "# Se o arquivo já existir, a mensagem de upload será exibida mas o usuário pode ignorar.\n",
        "print(\"Por favor, certifique-se que o arquivo .epub foi carregado. Se não, faça o upload agora.\")\n",
        "\n",
        "# Define o nome do arquivo .txt de saída\n",
        "text_filename = 'Klara e o Sol - Kazuo Ishiguro.txt'\n",
        "epub_filename = text_filename.replace('.txt', '.epub')\n",
        "\n",
        "# Check if the .txt file already exists to avoid unnecessary re-upload/conversion\n",
        "try:\n",
        "    with open(text_filename, 'r', encoding='utf-8') as f:\n",
        "        book_text = f.read()\n",
        "    print(f\"Arquivo '{text_filename}' já existe e foi carregado.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Arquivo '{text_filename}' não encontrado. Por favor, faça o upload do arquivo .epub...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"Nenhum arquivo .epub foi carregado. Não é possível prosseguir.\")\n",
        "        # Handle the error or exit gracefully\n",
        "        book_text = \"\"\n",
        "    else:\n",
        "        epub_filename = list(uploaded.keys())[0]\n",
        "        print(f\"Arquivo '{epub_filename}' carregado. Iniciando conversão...\")\n",
        "\n",
        "        # 2. Abrir o livro\n",
        "        book = epub.read_epub(epub_filename)\n",
        "        full_text = \"\"\n",
        "\n",
        "        # 3. Iterar por todos os \"documentos\" (capítulos) do livro\n",
        "        for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
        "            content = item.get_body_content()\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            text_only = soup.get_text(separator=' ', strip=True)\n",
        "            full_text += text_only + \"\\n\\n\"\n",
        "\n",
        "        # 4. Salvar o texto completo em um arquivo .txt\n",
        "        with open(text_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(full_text)\n",
        "        book_text = full_text\n",
        "        print(f\"\\nSucesso! O arquivo '{text_filename}' foi criado no seu ambiente Colab.\")\n",
        "\n",
        "\n",
        "# Re-create book_parts (from earlier step, e.g., cell d21440c9)\n",
        "print(\"Dividindo o texto do livro em partes lógicas...\")\n",
        "parts = re.split(r'\\n\\n+', book_text)\n",
        "parts = [part.strip() for part in parts if part.strip()]\n",
        "\n",
        "book_parts = []\n",
        "current_index = 0\n",
        "for part in parts:\n",
        "    start_index_part = book_text.find(part, current_index)\n",
        "    if start_index_part == -1:\n",
        "      start_index_part = book_text.find(part)\n",
        "      if start_index_part == -1:\n",
        "        continue\n",
        "    end_index_part = start_index_part + len(part)\n",
        "    book_parts.append({\n",
        "        'text': part,\n",
        "        'start_index': start_index_part,\n",
        "        'end_index': end_index_part\n",
        "    })\n",
        "    current_index = end_index_part\n",
        "\n",
        "print(f\"'book_parts' criada com {len(book_parts)} dicionários.\")\n",
        "\n",
        "# Now, proceed with the narrative extraction logic\n",
        "print(\"Detectando o início e o fim da narrativa principal no 'book_text'...\")\n",
        "\n",
        "# 1. Detectar o início da narrativa principal\n",
        "start_marker = \"Parte um\"\n",
        "start_index = book_text.find(start_marker)\n",
        "\n",
        "if start_index == -1:\n",
        "    print(f\"Erro: O marcador de início da narrativa '{start_marker}' não foi encontrado.\")\n",
        "    narrative_text = book_text # Fallback\n",
        "else:\n",
        "    start_index_of_narrative = start_index\n",
        "\n",
        "    # 2. Detectar o fim da narrativa principal\n",
        "    end_marker_part_index = 19\n",
        "    if end_marker_part_index < len(book_parts):\n",
        "        end_index_of_narrative = book_parts[end_marker_part_index]['start_index']\n",
        "    else:\n",
        "        print(\"Aviso: Índice da parte de fim da narrativa fora dos limites de book_parts. Usando o fim do livro.\")\n",
        "        end_index_of_narrative = len(book_text)\n",
        "\n",
        "    # 3. Extrair a narrativa pura\n",
        "    narrative_text = book_text[start_index_of_narrative:end_index_of_narrative].strip()\n",
        "\n",
        "print(f\"\\nComprimento do 'book_text' original: {len(book_text)} caracteres.\")\n",
        "print(f\"Comprimento do 'narrative_text' extraído: {len(narrative_text)} caracteres.\")\n",
        "\n",
        "print(\"\\nInício da narrativa extraída (primeiros 500 caracteres):\")\n",
        "print(narrative_text[:500])\n",
        "\n",
        "print(\"\\nFim da narrativa extraída (últimos 500 caracteres):\")\n",
        "print(narrative_text[-500:])\n",
        "\n",
        "\n",
        "print(\"\\nIniciando a extração de nomes de personagens do texto narrativo...\")\n",
        "\n",
        "# 1. Defina uma lista Python chamada main_characters contendo os nomes dos personagens principais do livro.\n",
        "main_characters = ['Klara', 'Josie', 'Rick', 'Melania Housekeeper', 'Padre', 'Mãe', 'Pai', 'Sr. Capaldi', 'O Coletor', 'B-2', 'B-1']\n",
        "\n",
        "# 2. Crie uma lista vazia chamada found_characters para armazenar os nomes dos personagens encontrados.\n",
        "found_characters = []\n",
        "\n",
        "# 3. Itere sobre cada nome na lista main_characters.\n",
        "for character_name in main_characters:\n",
        "    # 4. Para cada nome de personagem, verifique se ele está presente no narrative_text.\n",
        "    # Converta tanto o nome do personagem quanto o narrative_text para minúsculas antes de realizar a busca.\n",
        "    if character_name.lower() in narrative_text.lower():\n",
        "        # 5. Se o nome do personagem for encontrado no narrative_text, adicione-o à lista found_characters.\n",
        "        found_characters.append(character_name)\n",
        "\n",
        "# 6. Exiba a lista found_characters para verificar quais personagens foram identificados na narrativa.\n",
        "print(\"\\nPersonagens principais identificados na narrativa:\")\n",
        "if found_characters:\n",
        "    for character in found_characters:\n",
        "        print(f\"- {character}\")\n",
        "else:\n",
        "    print(\"Nenhum personagem principal da lista foi encontrado na narrativa. Verifique a lista ou o texto.\")\n",
        "\n",
        "# 7. Exiba o número total de personagens únicos encontrados.\n",
        "unique_found_characters = list(set(found_characters))\n",
        "print(f\"\\nTotal de personagens únicos encontrados: {len(unique_found_characters)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52215ee7"
      },
      "source": [
        "## Detectar Co-ocorrências de Personagens\n",
        "\n",
        "### Subtask:\n",
        "Percorrer o `narrative_text` e registrar as vezes em que pares de personagens aparecem juntos em um mesmo segmento de texto (por exemplo, dentro da mesma frase ou parágrafo). Isso será a base para as conexões do nosso grafo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad9f155e"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin detecting co-occurrences, I need to import the `nltk` library and download its 'punkt' tokenizer. Then, I will tokenize the `narrative_text` into sentences to process them individually for character presence, ensuring case-insensitive matching by converting the text to lowercase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c938cb3f"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"Baixando o tokenizador 'punkt' do NLTK (se necessário)...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "print(\"Tokenizador 'punkt' do NLTK pronto.\")\n",
        "\n",
        "print(\"Tokenizando o texto narrativo em frases...\")\n",
        "# Convert narrative_text to lowercase for case-insensitive processing\n",
        "lower_narrative_text = narrative_text.lower()\n",
        "sentences = sent_tokenize(lower_narrative_text, language='portuguese')\n",
        "\n",
        "print(f\"Narrativa dividida em {len(sentences)} frases.\")\n",
        "\n",
        "# Initialize co-occurrence dictionary\n",
        "# Using defaultdict to automatically handle new keys with a default value of 0\n",
        "co_occurrence_counts = defaultdict(int)\n",
        "\n",
        "# Prepare character names for case-insensitive search\n",
        "# Convert unique_found_characters to a set of lowercase names for efficient lookup\n",
        "# and store original names to map back if needed, but for co-occurrence, lowercase is fine\n",
        "lower_unique_found_characters = [char.lower() for char in unique_found_characters]\n",
        "\n",
        "print(\"Iniciando a detecção de co-ocorrências de personagens por frase...\")\n",
        "\n",
        "# Step 4-7: Iterate over each tokenized sentence\n",
        "for sentence in sentences:\n",
        "    present_characters = []\n",
        "    for character_name_orig in unique_found_characters: # Use original names for display later\n",
        "        # Check for the presence of the character name in the lowercase sentence\n",
        "        # Using word boundaries to avoid matching parts of other words (e.g., 'josie' in 'josiel')\n",
        "        if re.search(r'\\b' + re.escape(character_name_orig.lower()) + r'\\b', sentence):\n",
        "            present_characters.append(character_name_orig)\n",
        "\n",
        "    # If more than one character is found in the sentence, increment co-occurrence counts\n",
        "    if len(present_characters) > 1:\n",
        "        # Generate all unique pairs of characters found in this sentence\n",
        "        for i in range(len(present_characters)):\n",
        "            for j in range(i + 1, len(present_characters)):\n",
        "                char1 = present_characters[i]\n",
        "                char2 = present_characters[j]\n",
        "                # Ensure the pair is stored in alphabetical order to avoid duplicates (e.g., ('A', 'B') vs ('B', 'A'))\n",
        "                pair = tuple(sorted((char1, char2)))\n",
        "                co_occurrence_counts[pair] += 1\n",
        "\n",
        "print(\"Detecção de co-ocorrências concluída.\")\n",
        "\n",
        "# Step 8: Display the 10 greatest co-occurrence counts\n",
        "print(\"\\nAs 10 maiores contagens de co-ocorrência:\")\n",
        "sorted_co_occurrences = sorted(co_occurrence_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "for pair, count in sorted_co_occurrences[:10]:\n",
        "    print(f\"Par: {pair[0]} e {pair[1]} - Co-ocorrências: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "487fc496"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `nltk.downloader.DownloadError` is not the correct exception type to catch for a missing resource. The `LookupError` is raised when the resource is not found. The fix is to directly call `nltk.download('punkt', quiet=True)` if a `LookupError` occurs, as suggested by the traceback.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "968cc4c4"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "print(\"Baixando o tokenizador 'punkt' do NLTK (se necessário)...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "print(\"Tokenizador 'punkt' do NLTK pronto.\")\n",
        "\n",
        "print(\"Tokenizando o texto narrativo em frases...\")\n",
        "# Convert narrative_text to lowercase for case-insensitive processing\n",
        "lower_narrative_text = narrative_text.lower()\n",
        "sentences = sent_tokenize(lower_narrative_text, language='portuguese')\n",
        "\n",
        "print(f\"Narrativa dividida em {len(sentences)} frases.\")\n",
        "\n",
        "# Initialize co-occurrence dictionary\n",
        "# Using defaultdict to automatically handle new keys with a default value of 0\n",
        "co_occurrence_counts = defaultdict(int)\n",
        "\n",
        "# Prepare character names for case-insensitive search\n",
        "# Convert unique_found_characters to a set of lowercase names for efficient lookup\n",
        "# and store original names to map back if needed, but for co-occurrence, lowercase is fine\n",
        "lower_unique_found_characters = [char.lower() for char in unique_found_characters]\n",
        "\n",
        "print(\"Iniciando a detecção de co-ocorrências de personagens por frase...\")\n",
        "\n",
        "# Step 4-7: Iterate over each tokenized sentence\n",
        "for sentence in sentences:\n",
        "    present_characters = []\n",
        "    for character_name_orig in unique_found_characters: # Use original names for display later\n",
        "        # Check for the presence of the character name in the lowercase sentence\n",
        "        # Using word boundaries to avoid matching parts of other words (e.g., 'josie' in 'josiel')\n",
        "        if re.search(r'\\b' + re.escape(character_name_orig.lower()) + r'\\b', sentence):\n",
        "            present_characters.append(character_name_orig)\n",
        "\n",
        "    # If more than one character is found in the sentence, increment co-occurrence counts\n",
        "    if len(present_characters) > 1:\n",
        "        # Generate all unique pairs of characters found in this sentence\n",
        "        for i in range(len(present_characters)):\n",
        "            for j in range(i + 1, len(present_characters)):\n",
        "                char1 = present_characters[i]\n",
        "                char2 = present_characters[j]\n",
        "                # Ensure the pair is stored in alphabetical order to avoid duplicates (e.g., ('A', 'B') vs ('B', 'A'))\n",
        "                pair = tuple(sorted((char1, char2)))\n",
        "                co_occurrence_counts[pair] += 1\n",
        "\n",
        "print(\"Detecção de co-ocorrências concluída.\")\n",
        "\n",
        "# Step 8: Display the 10 greatest co-occurrence counts\n",
        "print(\"\\nAs 10 maiores contagens de co-ocorrência:\")\n",
        "sorted_co_occurrences = sorted(co_occurrence_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "for pair, count in sorted_co_occurrences[:10]:\n",
        "    print(f\"Par: {pair[0]} e {pair[1]} - Co-ocorrências: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c95f6d9f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed to find the 'punkt_tab' resource required for Portuguese sentence tokenization, leading to a `LookupError`. I will modify the NLTK download logic to explicitly download 'punkt' and 'punkt_tab' if a `LookupError` occurs, ensuring all necessary resources for Portuguese are available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1b17aa7"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "print(\"Baixando o tokenizador 'punkt' do NLTK (se necessário)...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "print(\"Tokenizador 'punkt' do NLTK pronto.\")\n",
        "\n",
        "print(\"Baixando o tokenizador 'punkt_tab' do NLTK para português (se necessário)...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab/portuguese') # Check for the Portuguese-specific resource\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "print(\"Tokenizador 'punkt_tab' do NLTK pronto para português.\")\n",
        "\n",
        "print(\"Tokenizando o texto narrativo em frases...\")\n",
        "# Convert narrative_text to lowercase for case-insensitive processing\n",
        "lower_narrative_text = narrative_text.lower()\n",
        "sentences = sent_tokenize(lower_narrative_text, language='portuguese')\n",
        "\n",
        "print(f\"Narrativa dividida em {len(sentences)} frases.\")\n",
        "\n",
        "# Initialize co-occurrence dictionary\n",
        "# Using defaultdict to automatically handle new keys with a default value of 0\n",
        "co_occurrence_counts = defaultdict(int)\n",
        "\n",
        "# Prepare character names for case-insensitive search\n",
        "# Convert unique_found_characters to a set of lowercase names for efficient lookup\n",
        "# and store original names to map back if needed, but for co-occurrence, lowercase is fine\n",
        "lower_unique_found_characters = [char.lower() for char in unique_found_characters]\n",
        "\n",
        "print(\"Iniciando a detecção de co-ocorrências de personagens por frase...\")\n",
        "\n",
        "# Step 4-7: Iterate over each tokenized sentence\n",
        "for sentence in sentences:\n",
        "    present_characters = []\n",
        "    for character_name_orig in unique_found_characters: # Use original names for display later\n",
        "        # Check for the presence of the character name in the lowercase sentence\n",
        "        # Using word boundaries to avoid matching parts of other words (e.g., 'josie' in 'josiel')\n",
        "        if re.search(r'\\b' + re.escape(character_name_orig.lower()) + r'\\b', sentence):\n",
        "            present_characters.append(character_name_orig)\n",
        "\n",
        "    # If more than one character is found in the sentence, increment co-occurrence counts\n",
        "    if len(present_characters) > 1:\n",
        "        # Generate all unique pairs of characters found in this sentence\n",
        "        for i in range(len(present_characters)):\n",
        "            for j in range(i + 1, len(present_characters)):\n",
        "                char1 = present_characters[i]\n",
        "                char2 = present_characters[j]\n",
        "                # Ensure the pair is stored in alphabetical order to avoid duplicates (e.g., ('A', 'B') vs ('B', 'A'))\n",
        "                pair = tuple(sorted((char1, char2)))\n",
        "                co_occurrence_counts[pair] += 1\n",
        "\n",
        "print(\"Detecção de co-ocorrências concluída.\")\n",
        "\n",
        "# Step 8: Display the 10 greatest co-occurrence counts\n",
        "print(\"\\nAs 10 maiores contagens de co-ocorrência:\")\n",
        "sorted_co_occurrences = sorted(co_occurrence_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "for pair, count in sorted_co_occurrences[:10]:\n",
        "    print(f\"Par: {pair[0]} e {pair[1]} - Co-ocorrências: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4761d4"
      },
      "source": [
        "## Construir o Grafo de Co-ocorrência\n",
        "\n",
        "### Subtask:\n",
        "Utilizar a biblioteca `networkx` para criar um grafo onde cada nó representa um personagem e cada aresta entre dois nós indica que esses dois personagens co-ocorreram no texto. O peso da aresta pode ser a frequência com que co-ocorreram.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96b6a08f"
      },
      "source": [
        "**Reasoning**:\n",
        "To build the co-occurrence graph, I need to import `networkx`, create a graph, add the characters as nodes, and then add edges between co-occurring characters with their respective counts as weights, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f4e9dfa"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "print(\"Criando o grafo de co-ocorrência...\")\n",
        "\n",
        "# 1. Crie um grafo vazio usando nx.Graph()\n",
        "G = nx.Graph()\n",
        "\n",
        "# 2. Adicione os personagens identificados como nós ao grafo G\n",
        "# unique_found_characters é a lista de personagens encontrados no narrative_text\n",
        "for character in unique_found_characters:\n",
        "    G.add_node(character)\n",
        "\n",
        "# 3. Itere sobre o dicionário co_occurrence_counts\n",
        "# 4. Para cada par de personagens e sua contagem de co-ocorrência, adicione uma aresta entre os dois personagens no grafo G.\n",
        "#    Defina o atributo `weight` da aresta com o valor da contagem de co-ocorrência.\n",
        "for pair, count in co_occurrence_counts.items():\n",
        "    char1, char2 = pair\n",
        "    G.add_edge(char1, char2, weight=count)\n",
        "\n",
        "# 5. Exiba o número de nós e arestas no grafo para verificar a sua criação.\n",
        "print(f\"Grafo criado com {G.number_of_nodes()} nós e {G.number_of_edges()} arestas.\")\n",
        "\n",
        "print(\"Nós do grafo (personagens):\", G.nodes())\n",
        "print(\"Arestas do grafo (co-ocorrências) com seus pesos (Top 10):\")\n",
        "# Sorting edges by weight to show the most significant co-occurrences\n",
        "sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
        "for edge in sorted_edges[:10]:\n",
        "    print(f\"  {edge[0]} -- {edge[1]} (Peso: {edge[2]['weight']})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b225421a"
      },
      "source": [
        "## Visualizar o Grafo da Rede\n",
        "\n",
        "### Subtask:\n",
        "Gerar uma visualização interativa do grafo de co-ocorrência de personagens usando a biblioteca `pyvis`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbafca5c"
      },
      "source": [
        "**Reasoning**:\n",
        "To visualize the co-occurrence graph interactively, I will first install the `pyvis` library, then create a `Network` instance, add nodes and edges from the existing `networkx` graph `G` with appropriate attributes, and finally save the visualization to an HTML file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e399839"
      },
      "source": [
        "print(\"Instalando a biblioteca pyvis...\")\n",
        "!pip install pyvis\n",
        "print(\"Biblioteca pyvis instalada. Gerando visualização interativa do grafo...\")\n",
        "\n",
        "from pyvis.network import Network\n",
        "\n",
        "# 1. Crie uma instância da classe Network\n",
        "# notebook=True é crucial para exibir corretamente no Google Colab\n",
        "# cdn_resources='remote' garante que os recursos JS/CSS são carregados da CDN\n",
        "net = Network(notebook=True, height='750px', width='100%', cdn_resources='remote')\n",
        "\n",
        "# 2. Itere sobre os nós do grafo G e adicione-os à rede pyvis\n",
        "for node in G.nodes():\n",
        "    # Usando o nome do personagem como id e label\n",
        "    net.add_node(node, label=node, title=node) # title adiciona tooltip ao passar o mouse\n",
        "\n",
        "# 3. Itere sobre as arestas do grafo G e adicione-as à rede pyvis\n",
        "for source, target, data in G.edges(data=True):\n",
        "    weight = data['weight']\n",
        "    net.add_edge(source, target, value=weight, title=f\"Co-ocorrências: {weight}\")\n",
        "\n",
        "# 4. Adicione opções de física para melhor visualização (opcional, mas recomendado)\n",
        "# net.toggle_physics(True) # A física é ativada por padrão com `notebook=True` para auto-layout\n",
        "\n",
        "# Você pode ajustar as opções de física para otimizar a visualização, por exemplo:\n",
        "net.set_options(\"\"\"\n",
        "var options = {\n",
        "  \"physics\": {\n",
        "    \"enabled\": true,\n",
        "    \"barnesHut\": {\n",
        "      \"gravitationalConstant\": -2000,\n",
        "      \"centralGravity\": 0.3,\n",
        "      \"springLength\": 95,\n",
        "      \"springConstant\": 0.04,\n",
        "      \"damping\": 0.09,\n",
        "      \"avoidOverlap\": 0\n",
        "    },\n",
        "    \"maxVelocity\": 50,\n",
        "    \"minVelocity\": 0.1,\n",
        "    \"solver\": \"barnesHut\",\n",
        "    \"stabilization\": {\n",
        "      \"enabled\": true,\n",
        "      \"iterations\": 1000,\n",
        "      \"updateInterval\": 25,\n",
        "      \"fit\": true\n",
        "    },\n",
        "    \"timestep\": 0.5\n",
        "  }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# 5. Salve o gráfico interativo em um arquivo HTML\n",
        "html_file_path = 'klara_co_occurrence_graph.html'\n",
        "net.show(html_file_path)\n",
        "\n",
        "print(f\"Visualização interativa do grafo salva em '{html_file_path}'.\")\n",
        "print(\"Você pode abrir este arquivo HTML em seu navegador para explorar o grafo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0eafc43"
      },
      "source": [
        "## Interpretar a Análise da Rede\n",
        "\n",
        "### Subtask:\n",
        "Analisar a estrutura do grafo gerado, identificando os personagens mais centrais (com mais conexões), os grupos de personagens que interagem frequentemente e o que essas relações podem revelar sobre a dinâmica da história e a jornada de Klara.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c294e2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Character Extraction and Narrative Segmentation**: The main narrative text was successfully extracted, spanning from \"Parte um\" to the beginning of promotional materials. From this narrative, six key characters were identified: Klara, Josie, Rick, Mãe, Pai, and Sr. Capaldi.\n",
        "*   **Character Co-occurrence Detection**: The narrative was tokenized into 5740 sentences. Analysis of these sentences revealed significant co-occurrence patterns between characters. The most frequent co-occurring pairs were \"Josie\" and \"Mãe\" (143 times), followed by \"Josie\" and \"Rick\" (122 times).\n",
        "*   **Co-occurrence Graph Construction**: A graph was successfully constructed with 6 nodes (representing the identified characters) and 15 edges (representing co-occurrences). The edges are weighted by their co-occurrence frequency, with \"Mãe -- Josie\" having the strongest connection (weight: 143).\n",
        "*   **Interactive Graph Visualization**: An interactive visualization of the character co-occurrence graph was generated using `pyvis` and saved as an HTML file. This visualization allows for an intuitive exploration of the relationships between characters based on their interactions within the narrative.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Relationship Centrality**: The high co-occurrence counts for \"Josie\" with \"Mãe\" and \"Rick\" suggest that Josie is a central figure in the narrative, around whom many interactions revolve. This aligns with Klara's role as an Artificial Friend observing Josie. Further analysis of Klara's direct co-occurrences would be valuable.\n",
        "*   **Deepen Network Analysis**: To understand the dynamics of the story and Klara's journey more fully, compute centrality measures (e.g., degree centrality, betweenness centrality) for each character in the graph. This will quantify their importance and influence within the network of interactions.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}